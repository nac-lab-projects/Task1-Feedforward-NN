{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a44be76",
   "metadata": {},
   "source": [
    "### *Step 1: Imports*\n",
    "\n",
    "*Import necessary modules and functions. `data_utils` contains preprocessing functions, `model` defines our neural network, a   `train_utils` handles the training loop, `test_utils` handles testing, and `predict` provides prediction functionality.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3edf5a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from data_utils import load_and_tokenize, build_vocab, prepare_data\n",
    "from model import FeedforwardNN\n",
    "from train_utils import train_model\n",
    "from test_utils import test_model \n",
    "from predict import predict_next_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f2cc46",
   "metadata": {},
   "source": [
    "### *Step 2: Load Dataset*\n",
    "\n",
    "*Specify the path to the Penn Treebank dataset and load the training and validation sentences using our `load_and_tokenize` function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84cbbbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = load_and_tokenize(\"ptbdataset/ptb.train.txt\")\n",
    "val_sentences   = load_and_tokenize(\"ptbdataset/ptb.valid.txt\")\n",
    "test_sentences  = load_and_tokenize(\"ptbdataset/ptb.test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f59e06",
   "metadata": {},
   "source": [
    "### *Step 3: Build Vocabulary and Prepare Data*\n",
    "\n",
    "*Create word-to-index and index-to-word mappings using `build_vocab`. Then, `prepare_data` converts tokenized sentences into numerical sequences for model training. Inputs are sequences of words, and outputs are the next words.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b323319",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word = build_vocab(train_sentences)\n",
    "train_inputs, train_outputs, max_seq_len = prepare_data(train_sentences, word_to_index)\n",
    "val_inputs, val_outputs, _ = prepare_data(val_sentences, word_to_index, max_len=max_seq_len)\n",
    "test_inputs, test_outputs, _ = prepare_data(test_sentences, word_to_index, max_len=max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9923b53c",
   "metadata": {},
   "source": [
    "### *Step 4: Initialize Model*\n",
    "\n",
    "*Define the neural network with input size equal to the sequence length, two hidden layers (128 and 64 neurons), and output size equal to the vocabulary size. We then create an instance of `FeedforwardNN`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "debb86e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = max_seq_len - 1\n",
    "embedding_dim = 50\n",
    "hidden_size1, hidden_size2 = 128, 64\n",
    "vocab_size = len(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f744d1f",
   "metadata": {},
   "source": [
    "### *Step 5: Train the Model*\n",
    "\n",
    "*Train the neural network using `train_model`, which handles batching, loss computation, backpropagation, and validation. For simplicity, we predict only the first next word in each sequence.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e3ebb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate with correct signature\n",
    "model = FeedforwardNN(seq_len=max_seq_len-1, \n",
    "                      embedding_dim=embedding_dim, \n",
    "                      hidden1=hidden_size1, \n",
    "                      hidden2=hidden_size2, \n",
    "                      vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97ec2fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 4.9836\n",
      "Validation Loss: 3.6094\n",
      "Epoch 2/5, Loss: 2.4653\n",
      "Validation Loss: 2.6687\n",
      "Epoch 3/5, Loss: 1.1740\n",
      "Validation Loss: 2.2921\n",
      "Epoch 4/5, Loss: 0.4830\n",
      "Validation Loss: 2.2266\n",
      "Epoch 5/5, Loss: 0.1631\n",
      "Validation Loss: 2.4360\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_inputs, train_outputs, val_inputs, val_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7caa9d6",
   "metadata": {},
   "source": [
    "*Observations from the Loss Values*\n",
    "\n",
    "*Training Loss Trend*\n",
    "\n",
    "*Epoch 1 → 4.9836 → Epoch 5 → 0.1631*\n",
    "\n",
    "*The training loss decreases sharply across epochs, which indicates that the model is learning the training data well and fitting it accurately.*\n",
    "\n",
    "*Validation Loss Trend*\n",
    "\n",
    "*Starts at 3.6094 (Epoch 1) and ends at 2.4360 (Epoch 5).*\n",
    "\n",
    "*The validation loss decreases initially but slightly increases at the last epoch, suggesting the model might be starting to overfit the training data.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b92d5fc",
   "metadata": {},
   "source": [
    "### *Step 6: Save the trained model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f18b6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a86ed1b",
   "metadata": {},
   "source": [
    "### *Step 7: Evaluate the model on a test set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89ab52ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.8733\n"
     ]
    }
   ],
   "source": [
    "# After training the model\n",
    "test_model(model, test_inputs, test_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e81685a",
   "metadata": {},
   "source": [
    "*Test Loss Observation*\n",
    "\n",
    "*Test Loss: 1.8733*\n",
    "\n",
    "*The test loss is lower than the initial validation loss and indicates that the model generalizes reasonably well to unseen data, though there is still a gap compared to the final training loss, suggesting some overfitting.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fe3025",
   "metadata": {},
   "source": [
    "### *Step 8: Make predictions / inference*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98550460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next word: banknote\n"
     ]
    }
   ],
   "source": [
    "# Example: use the first training sentence as seed  \n",
    "seed_seq = train_inputs[0]  # first training sentence\n",
    "next_word = predict_next_word(model, seed_seq, word_to_index, index_to_word)\n",
    "print(\"Predicted next word:\", next_word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myenv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
