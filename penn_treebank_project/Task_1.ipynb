{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aedbb6c",
   "metadata": {},
   "source": [
    "## *Task 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafa7751",
   "metadata": {},
   "source": [
    "### *Description:* Date: September 25, 2025\n",
    "\n",
    "*Assigned tasks included:*\n",
    "\n",
    "*1.Developing a conceptual understanding of neural networks and the backpropagation algorithm.*\n",
    "\n",
    "*2.Implementing a simple prototype feedforward neural network with:*\n",
    "\n",
    "*1 input layer, 2 hidden layers, and 1 output layer*\n",
    "\n",
    "*Using the Penn Treebank dataset from Kaggle, with the data already pre-split for training and testing.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9093344",
   "metadata": {},
   "source": [
    "\n",
    "# *Neural Networks: A Conceptual Overview*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bf8700",
   "metadata": {},
   "source": [
    "#### *1.Developing a conceptual understanding of neural networks and the backpropagation algorithm.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478bde8b",
   "metadata": {},
   "source": [
    "Neural networks are a type of supervised machine learning algorithm. Unlike other supervised ML models, which are often categorized as regression or classification, neural networks can handle `both regression and classification tasks`.\n",
    "\n",
    "A neural network is built from:\n",
    "\n",
    "- **Input layer:** the raw data (e.g., pixel values of a digit).  \n",
    "- **Hidden layers:** where the network learns internal patterns. These can be shallow (few layers) or deep (many layers).  \n",
    "- **Output layer:** the prediction (e.g., digit label 0â€“9, or a real value for regression).  \n",
    "\n",
    "The motivation comes from how the human brain works. Just like the brain receives signals from sensory organs, processes them across many neurons, and produces an action, neural networks take input, process it in hidden layers of neurons, and produce an output.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Example: Digit Recognition (MNIST)\n",
    "\n",
    "- **Input:** pixel values of a handwritten digit (0â€“9).  \n",
    "- **Hidden layers:** detect patterns like curves, edges, or corners.  \n",
    "- **Output:** probability distribution across digits (e.g., 90% â€œ9â€, 5% â€œ8â€, etc.).\n",
    "\n",
    "\n",
    "## `Key idea:`\n",
    "\n",
    "Instead of processing all information blindly, a PC-based network learns more efficiently by concentrating on unexpected or surprising signals, similar to how the brain focuses on deviations from expectations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e60096",
   "metadata": {},
   "source": [
    "### *2. Core Components*\n",
    "\n",
    "- **Weights:** *represent the strength of the connection between neurons in adjacent layers.*  \n",
    "- **Biases:** *thresholds that shift activation outputs.*  \n",
    "- **Activation functions** (e.g., sigmoid, ReLU): *introduce non-linearity and decide whether a neuron activates.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d354928",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### *ðŸ”¹ The Training Problem*\n",
    "\n",
    "*We want the output to match the target. The mismatch is measured by the `cost function`:*\n",
    "\n",
    "$$\n",
    "C = \\frac{1}{2} (y - \\hat{y})^2\n",
    "$$\n",
    "\n",
    "* *If prediction = target, cost = 0.*\n",
    "* *The closer the output is to the target, the lower the cost.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a489af",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# *ðŸ”¹ Backpropagation + Calculus (The Learning Mechanism)*\n",
    "\n",
    "**Key Idea:**\n",
    "\n",
    "* *Every parameter (weight, bias) influences the cost indirectly through many layers.*\n",
    "* *To adjust them, we must ask: if I make a small change in this weight, how does the cost change?*\n",
    "\n",
    "---\n",
    "\n",
    "**Where Calculus Comes In:**\n",
    "\n",
    "* *The derivative tells us the effect of a tiny change (slope).*\n",
    "* *Using the chain rule, we propagate this sensitivity backward layer by layer.*\n",
    "\n",
    "---\n",
    "\n",
    "**For a single neuron, the contribution is:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w} = \\frac{\\partial C}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w}\n",
    "$$\n",
    "\n",
    "* *This links cost â†’ activation â†’ neuron input â†’ weight.*\n",
    "* *It answers: How does a tiny change in weight or bias affect the overall cost?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa2d27d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# *ðŸ”¹ Gradient Descent (Parameter Update)*\n",
    "\n",
    "Once we have the *derivatives (gradients)*, we update the parameters:\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\eta \\frac{\\partial C}{\\partial w}, \\quad\n",
    "b \\leftarrow b - \\eta \\frac{\\partial C}{\\partial b}\n",
    "$$\n",
    "\n",
    "* (*$\\eta$* = *learning rate*).\n",
    "* This process *gradually reduces the cost function* (*$C$*), until predictions align with targets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a859b1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## *Perceptron and Sigmoid Neurons*\n",
    "\n",
    "**Perceptron:**\n",
    "$$\n",
    "z = \\sum_i w_i x_i + b\n",
    "$$\n",
    "$$\n",
    "\\text{Output} =\n",
    "\\begin{cases}\n",
    "1 & z > 0 \\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "*Limitation: Only works for linearly separable problems.*\n",
    "\n",
    "**Sigmoid Neuron:**\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "*Outputs continuous values between 0 and 1.*\n",
    "*Smooth and differentiable.*\n",
    "\n",
    "---\n",
    "\n",
    "## *Summary of Training*\n",
    "\n",
    "1. *Input raw data.*\n",
    "2. *Forward pass through layers.*\n",
    "3. *Compute cost.*\n",
    "4. *Backpropagate error.*\n",
    "5. *Update weights/biases with gradient descent.*\n",
    "6. *Repeat until convergence.*\n",
    "\n",
    "---\n",
    "\n",
    "## *Key Takeaways*\n",
    "\n",
    "* *Neural networks model complex patterns.*\n",
    "* *Backpropagation + gradient descent = core training method.*\n",
    "* *Activation functions add non-linearity.*\n",
    "* *Mini-batch SGD is widely used for efficiency.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b0bb3e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### *Key Concepts from the Notebook*\n",
    "\n",
    "* *Tensor Basics:*\n",
    "\n",
    "  * *Creating tensors, specifying data types, and reshaping.*\n",
    "  * *Operations like addition, subtraction, and slicing.*\n",
    "\n",
    "* *Autograd:*\n",
    "\n",
    "  * *Automatic differentiation for gradient computation.*\n",
    "  * *Using `.backward()` for backpropagation.*\n",
    "\n",
    "* *Model, Loss, and Optimizer:*\n",
    "\n",
    "  * *Defining a model class with `torch.nn.Module`.*\n",
    "  * *Using loss functions like `MSELoss` or `CrossEntropyLoss`.*\n",
    "  * *Optimizers like SGD or Adam.*\n",
    "\n",
    "* *Training Loop:*\n",
    "\n",
    "  * *Forward pass: Compute predictions.*\n",
    "  * *Backward pass: Compute gradients.*\n",
    "  * *Update weights using the optimizer.*\n",
    "\n",
    "* *Evaluation:*\n",
    "\n",
    "  * *Testing the model and calculating accuracy.*\n",
    "\n",
    "* *Neural Network:*\n",
    "\n",
    "  * *Building a feedforward neural network with hidden layers.*\n",
    "  * *Using activation functions like ReLU.*\n",
    "\n",
    "* *Dataset and DataLoader:*\n",
    "\n",
    "  * *Loading datasets and batching using `torch.utils.data.DataLoader`.*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "742ca354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download NLTK data required for tokenization\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b463c93",
   "metadata": {},
   "source": [
    "### *1.Dataset Preprocessing:*\n",
    "\n",
    "***Tokenization**: Loaded and tokenized the dataset files (train, test, validation).*\n",
    "\n",
    "***Word-to-Index Mapping**: Created mappings for words to indices and vice versa.*\n",
    "\n",
    "***Input-Output Preparation**: Prepared input-output pairs for training, testing, and validation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4bdc9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the dataset folders\n",
    "from pathlib import Path\n",
    "base_path = Path(\"C:/Users/Snit Kahsay/Desktop/my_project/NAC-_LAB_PROJECT/penn_treebank/ptbdataset\")\n",
    "\n",
    "train_file = base_path / \"ptb.train.txt\"\n",
    "test_file  = base_path / \"ptb.test.txt\"\n",
    "val_file   = base_path / \"ptb.valid.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd6caa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Function to read and tokenize a text file\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def load_and_tokenize(file_path):\n",
    "    sentences = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            tokens = word_tokenize(line.strip())\n",
    "            if tokens:  # skip empty lines\n",
    "                sentences.append(tokens)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2ab16a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sentences: 42068\n",
      "Test sentences: 3761\n",
      "Validation sentences: 3370\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Load all datasets\n",
    "train_sentences = load_and_tokenize(train_file)\n",
    "test_sentences  = load_and_tokenize(test_file)\n",
    "val_sentences   = load_and_tokenize(val_file)\n",
    "\n",
    "print(f\"Train sentences: {len(train_sentences)}\")\n",
    "print(f\"Test sentences: {len(test_sentences)}\")\n",
    "print(f\"Validation sentences: {len(val_sentences)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84360f1",
   "metadata": {},
   "source": [
    "####  *Preparing Inputâ€“Output Pairs for Training*\n",
    "\n",
    "##### *Purpose*  \n",
    "*This step converts tokenized sentences into numerical sequences that a neural network can process.  \n",
    "The model will learn to predict the next word given previous words.*\n",
    "\n",
    "---\n",
    "\n",
    "### *Key Steps*\n",
    "\n",
    "*1. Build Vocabulary*  \n",
    "*- Extract all unique words from the training sentences.*  \n",
    "*- Assign special tokens: `<PAD>` for padding (ignored during training) and `<UNK>` for unknown words.*  \n",
    "*- All other words receive consecutive indices.*\n",
    "\n",
    "*2. Convert Words to Indices*  \n",
    "*- Map each token in a sentence to its index in the vocabulary.*  \n",
    "*- Words not in the vocabulary are mapped to `<UNK>`.*\n",
    "\n",
    "*3. Fix Sequence Length*  \n",
    "*- Ensure all sequences are the same length.*  \n",
    "*- Short sequences are padded with `<PAD>`.*  \n",
    "\n",
    "*4. Create Inputâ€“Output Pairs*  \n",
    "*- Input sequences contain all tokens except the last one.*  \n",
    "*- Output sequences contain all tokens except the first one.*  \n",
    "*- This setup allows the model to learn next-word prediction.*\n",
    "\n",
    "---\n",
    "\n",
    "#### *Why This Is Important*  \n",
    "*- Converts raw text into supervised training data for neural networks.*  \n",
    "*- Handles unknown words safely.*  \n",
    "*- Ensures uniform sequence lengths for efficient batch processing.*  \n",
    "*- Prepares the dataset for the language modeling task.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a78b9c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create word-to-index mapping \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "540bd559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the dataset to create a vocabulary\n",
    "all_tokens = [token for sentence in train_sentences for token in sentence]\n",
    "vocab = set(all_tokens)  # unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6841a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word-to-index and index-to-word mappings\n",
    "# Now build word_to_index\n",
    "word_to_index = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for idx, word in enumerate(sorted(vocab), start=2):\n",
    "    word_to_index[word] = idx\n",
    "\n",
    "# Reverse mapping\n",
    "index_to_word = {idx: word for word, idx in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd2b8707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare input-output pairs\n",
    "def prepare_data(sentences, word_to_index, max_len=None):\n",
    "    inputs, outputs = [], []\n",
    "    \n",
    "    # Determine max_len dynamically if not provided\n",
    "    if max_len is None:\n",
    "        max_len = max(len(s) for s in sentences)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Map words to indices; unknown words â†’ 1 (<UNK>)\n",
    "        indices = [word_to_index.get(token, 1) for token in sentence]\n",
    "        \n",
    "        # Truncate or pad to max_len\n",
    "        if len(indices) > max_len:\n",
    "            indices = indices[:max_len]\n",
    "        else:\n",
    "            indices += [0] * (max_len - len(indices))  # pad with 0 (<PAD>)\n",
    "        \n",
    "        # Input = all tokens except last, Output = all tokens except first\n",
    "        inputs.append(indices[:-1])\n",
    "        outputs.append(indices[1:])\n",
    "    \n",
    "    return np.array(inputs), np.array(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7315467a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10007\n",
      "Sample input: [ 239  810  953 1327 1478 1693 3774 3923 4070 4383 4734 4926 5574 5737\n",
      " 5882 7097 7181 7372 7775 8206 8304 8481 8822 9665    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "Sample output: [ 810  953 1327 1478 1693 3774 3923 4070 4383 4734 4926 5574 5737 5882\n",
      " 7097 7181 7372 7775 8206 8304 8481 8822 9665    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Apply to train, test, and validation sets\n",
    "train_inputs, train_outputs = prepare_data(train_sentences, word_to_index)\n",
    "test_inputs, test_outputs   = prepare_data(test_sentences, word_to_index)\n",
    "val_inputs, val_outputs     = prepare_data(val_sentences, word_to_index)\n",
    "\n",
    "print(f\"Vocabulary size: {len(word_to_index)}\")\n",
    "print(f\"Sample input: {train_inputs[0]}\")\n",
    "print(f\"Sample output: {train_outputs[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67ccd1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10007\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_to_index)\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2805914",
   "metadata": {},
   "source": [
    "## *Building the Neural Network:*\n",
    "\n",
    "*Define the architecture of the feedforward neural network with 1 input layer, 2 hidden layers, and 1 output layer.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b733bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fcf07e",
   "metadata": {},
   "source": [
    "### *2.Model Definition*\n",
    "\n",
    "*Defined a Feedforward Neural Network with:*\n",
    "\n",
    "*Input layer â†’ input_size = 116 (based on your features).*\n",
    "\n",
    "*Hidden layers â†’ two (128 â†’ 64) with ReLU activations.*\n",
    "\n",
    "*Output layer â†’ output_size = vocab_size (predicts next word from vocabulary).*\n",
    "\n",
    "``` python\n",
    "Input â†’ Linear(116â†’128) â†’ ReLU â†’ Linear(128â†’64) â†’ ReLU â†’ Linear(64â†’vocab_size)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "96a8b83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedforwardNN(\n",
      "  (fc1): Linear(in_features=116, out_features=128, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc3): Linear(in_features=64, out_features=10007, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the feedforward neural network\n",
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(FeedforwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)  # First hidden layer\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)  # Second hidden layer\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)    # now [batch_size, seq_len, vocab_size]\n",
    "\n",
    "        return x\n",
    "\n",
    "# Example: Initialize the model\n",
    "input_size = 116  # Updated input size to match the input tensor size\n",
    "hidden_size1 = 128\n",
    "hidden_size2 = 64\n",
    "output_size = vocab_size\n",
    "model = FeedforwardNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba5419f",
   "metadata": {},
   "source": [
    "### *3.Training Setup*\n",
    "\n",
    "*You implemented the loop:*\n",
    "\n",
    "*Convert inputs â†’ tensor of shape [1, seq_len]*\n",
    "\n",
    "*Forward pass â†’ model prediction*\n",
    "\n",
    "*Debugging tensor shapes at each step (outputs.shape, targets.shape)*\n",
    "\n",
    "*Flattening for CrossEntropyLoss:*\n",
    "\n",
    "*outputs.view(-1, vocab_size)*\n",
    "\n",
    "*targets.view(-1)*\n",
    "\n",
    "*Backward pass (loss.backward)*\n",
    "\n",
    "*Optimizer step (weights updated)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305402bb",
   "metadata": {},
   "source": [
    "####  *Step 1 â€” Create a Custom Dataset*\n",
    "*Since we  already have train_inputs and train_outputs prepared as lists/tensors, letâ€™s wrap them:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "196cc819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PTBDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert to tensors here\n",
    "        x = torch.tensor(self.inputs[idx], dtype=torch.float)\n",
    "        y = torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aadbba0",
   "metadata": {},
   "source": [
    "#### *Step 2 â€” Build DataLoaders*\n",
    "\n",
    "*Now, use DataLoader to create mini-batches:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b62a97e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32  # You can tune this\n",
    "train_dataset = PTBDataset(train_inputs, train_outputs)\n",
    "val_dataset = PTBDataset(val_inputs, val_outputs)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8a6bff",
   "metadata": {},
   "source": [
    "#### *Step 3 â€” Modify Training Loop*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc299678",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_inputs, batch_targets in train_loader:\n",
    "    outputs = model(batch_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d5ee51d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 10007])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(batch_inputs)\n",
    "print(outputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc8baee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 10007]) torch.Size([20, 116])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.shape, batch_targets.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5dcdec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert your data to tensors (if not already)\n",
    "train_inputs = torch.tensor(train_inputs, dtype=torch.float32)\n",
    "train_outputs = torch.tensor(train_outputs, dtype=torch.long)\n",
    "\n",
    "# Now you can reshape\n",
    "train_inputs = train_inputs.view(-1, input_size)\n",
    "train_outputs = train_outputs.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8578f4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "14ccbd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 8.1041\n",
      "Epoch [2/5], Loss: 7.9707\n",
      "Epoch [3/5], Loss: 6.7859\n",
      "Epoch [4/5], Loss: 6.8123\n",
      "Epoch [5/5], Loss: 7.1090\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_inputs, batch_targets in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(batch_inputs)   # [batch_size, vocab_size]\n",
    "\n",
    "        # Keep only one label per sequence\n",
    "        batch_targets = batch_targets[:, 0]   # [batch_size]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153d895b",
   "metadata": {},
   "source": [
    "#### *Step 4 â€” Validation*\n",
    "*After training epoch, evaluate on val_loader without gradients:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "006cfa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     val_loss = 0\n",
    "#     for val_inputs, val_targets in val_loader:\n",
    "#         outputs = model(val_inputs)\n",
    "#         outputs = outputs.view(-1, vocab_size)\n",
    "#         val_targets = val_targets.view(-1)\n",
    "#         val_loss += criterion(outputs, val_targets).item()\n",
    "#     print(f\"Validation Loss: {val_loss / len(val_loader):.4f}\")\n",
    "# model.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myenv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
