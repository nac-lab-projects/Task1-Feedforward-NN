{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c0a5e23",
   "metadata": {},
   "source": [
    "## *Data Preprocessing for Next-Word Prediction*\n",
    "\n",
    "*This notebook handles the preprocessing of the Penn Treebank dataset.*\n",
    "\n",
    "*It includes loading the dataset, tokenizing the sentences, building the vocabulary,and converting text into numerical sequences ready for model training.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "041b5494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5862036b",
   "metadata": {},
   "source": [
    "### *1. Load and Tokenize the Dataset*\n",
    "\n",
    "*This function reads the text files (train, validation, and test), splits them into sentences and words, and returns tokenized text as lists of tokens.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d66f687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and preprocess dataset\n",
    "def load_and_tokenize(file_path):\n",
    "    sentences = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:  # as f gives us a file handle (you can think of it as a variable representing the opened file).\n",
    "        for line in f:\n",
    "            tokens = word_tokenize(line.strip())\n",
    "            if tokens:\n",
    "                sentences.append(tokens)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d92941a",
   "metadata": {},
   "source": [
    "### *2. Build Vocabulary*\n",
    "\n",
    "*This step builds a mapping between words and indices.*\n",
    "- `word_to_index`: maps each unique word to an integer.\n",
    "- `index_to_word`: reverse mapping, used for decoding predictions.\n",
    "\n",
    "*We also reserve special tokens like `<PAD>` and `<UNK>` for padding and unknown words.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afa953dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences):\n",
    "    all_tokens = [token for sent in sentences for token in sent]    #This flattens the list of lists.\n",
    "    vocab = set(all_tokens)\n",
    "    word_to_index = {\"<PAD>\":0, \"<UNK>\":1}\n",
    "    for idx, word in enumerate(sorted(vocab), start=2):\n",
    "        word_to_index[word] = idx    #word_to_index:when preparing data for training.\n",
    "    index_to_word = {idx: word for word, idx in word_to_index.items()}   #index_to_word when translating predictions back to text.\n",
    "    return word_to_index, index_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6019f6",
   "metadata": {},
   "source": [
    "### *3. Prepare Data for Model Training*\n",
    "\n",
    "*This function converts tokenized sentences into numerical sequences using the vocabulary.*\n",
    "\n",
    "*Each sequence is padded or truncated to have the same length.It also splits inputs and outputs for next-word prediction:*\n",
    "- *Input: [word1, word2, ..., word_(n-1)]*\n",
    "- *Output: [word2, word3, ..., word_n]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "764fd67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(sentences, word_to_index, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = max(len(s) for s in sentences)\n",
    "    inputs, outputs = [], []\n",
    "    for sentence in sentences:\n",
    "        indices = [word_to_index.get(token, 1) for token in sentence]\n",
    "        if len(indices) > max_len:\n",
    "            indices = indices[:max_len]\n",
    "        else:\n",
    "            indices += [0]*(max_len - len(indices))\n",
    "        inputs.append(indices[:-1])\n",
    "        outputs.append(indices[1:])\n",
    "    return np.array(inputs), np.array(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myenv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
